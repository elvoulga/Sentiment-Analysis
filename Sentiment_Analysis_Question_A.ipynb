{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Question A - Voulgari Eleni - A.M. 17005\n",
    "\n",
    "## Create and Evaluate Classifier for Tweets\n",
    "\n",
    "### Step 1: Prepare Project\n",
    "\n",
    "   1. Load libraries\n",
    "   2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed.\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training sets from years 2013, 2015 and 2016 into pandas dataframes and concatenate them into one large training \n",
    "# dataframe and also load dev sets and concatenate them in one develop set and the test set.\n",
    "cols = ['id','sentiment','text']\n",
    "file2013 = pd.read_csv(\"./sets/twitter-2013train-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "file2015 = pd.read_csv(\"./sets/twitter-2015train-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "file2016 = pd.read_csv(\"./sets/twitter-2016train-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "test2016 = pd.read_csv(\"./sets/twitter-2016test-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "dev = pd.read_csv(\"./sets/twitter-2016dev-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "devtest = pd.read_csv(\"./sets/twitter-2016devtest-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "training = pd.concat([file2013, file2015, file2016])\n",
    "develop = pd.concat([dev, devtest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Problem\n",
    "\n",
    "##### What is your task? What are your goals? What do you want to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to create and evaluate a supervised classifier for tweets according to their polarity (positive, negative, neutral). The goal is to be able to recognize the sentiment behind a tweeter text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory Analysis\n",
    "\n",
    "##### Understand your data: Take a “peek” of your data, answer basic questions about the dataset. Summarise your data. Explore descriptive statistics and visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "                   id sentiment  \\\n",
      "0  264183816548130816  positive   \n",
      "1  263405084770172928  negative   \n",
      "2  262163168678248449  negative   \n",
      "\n",
      "                                                text  \n",
      "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
      "1                                      Not Available  \n",
      "2                                      Not Available  \n",
      "\n",
      "\n",
      "Development dataset:\n",
      "                   id sentiment  \\\n",
      "0  638060586258038784   neutral   \n",
      "1  638061181823922176  positive   \n",
      "2  638083821364244480   neutral   \n",
      "\n",
      "                                                text  \n",
      "0  05 Beat it - Michael Jackson - Thriller (25th ...  \n",
      "1  Jay Z joins Instagram with nostalgic tribute t...  \n",
      "2  Michael Jackson: Bad 25th Anniversary Edition ...  \n",
      "\n",
      "\n",
      "Test dataset:\n",
      "                   id sentiment  \\\n",
      "0  619950566786113536   neutral   \n",
      "1  619969366986235905   neutral   \n",
      "2  619971047195045888  negative   \n",
      "\n",
      "                                                text  \n",
      "0  Picturehouse's, Pink Floyd's, 'Roger Waters: T...  \n",
      "1  Order Go Set a Watchman in store or through ou...  \n",
      "2                                      Not Available  \n"
     ]
    }
   ],
   "source": [
    "# Check what the datasets looks like.\n",
    "print \"Training dataset:\\n\", training.head(3)\n",
    "print \"\\n\"\n",
    "print \"Development dataset:\\n\", develop.head(3)\n",
    "print \"\\n\"\n",
    "print \"Test dataset:\\n\", test2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (16045, 3)\n",
      "Development dataset shape: (3947, 3)\n",
      "Test dataset shape: (20342, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the datasets, thus their size.\n",
    "print \"Training dataset shape:\", training.shape\n",
    "print \"Development dataset shape:\", develop.shape\n",
    "print \"Test dataset shape:\", test2016.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training null values:\n",
      "id           False\n",
      "sentiment    False\n",
      "text         False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "Development null values:\n",
      "id           False\n",
      "sentiment    False\n",
      "text         False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "Test null values:\n",
      "id           False\n",
      "sentiment    False\n",
      "text         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any null values in the datasets.\n",
    "print \"Training null values:\\n\", pd.isnull(training).any()\n",
    "print \"\\n\"\n",
    "print \"Development null values:\\n\", pd.isnull(develop).any()\n",
    "print \"\\n\"\n",
    "print \"Test null values:\\n\", pd.isnull(test2016).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    2373\n",
       "neutral     6829\n",
       "positive    6843\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of instances that have each sentiment label in the training set.\n",
    "training.groupby('sentiment').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Data\n",
    "\n",
    "##### Data Cleaning/Data Wrangling/Collect more data (if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some useful variables like removing negative words from stopwords, a lemmatizer and the vectorizers.\n",
    "old_stop_words = set(stopwords.words('english'))\n",
    "no_words = ['not', 'no', 'nor', 'against']\n",
    "new_stop_words = [word for word in old_stop_words if word not in no_words]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (11918, 3)\n",
      "Development shape: (3083, 3)\n",
      "Test shape: (15437, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove the instances where the tweet text is not available\n",
    "training = training[training.text != 'Not Available']\n",
    "develop = develop[develop.text != 'Not Available']\n",
    "test2016 = test2016[test2016.text != 'Not Available']\n",
    "\n",
    "print \"Training shape:\", training.shape\n",
    "print \"Development shape:\", develop.shape\n",
    "print \"Test shape:\", test2016.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove column id which is not needed for the purpose of sentiment analysis\n",
    "training = training.drop(['id'], axis=1)\n",
    "develop = develop.drop(['id'], axis=1)\n",
    "test2016 = test2016.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    1658\n",
       "neutral     5143\n",
       "positive    5117\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of instances that have each sentiment label in the training set again. As we can see the instances \n",
    "# with 'negative' sentiment are a lot less than the other two which makes our dataset somehow unbalanced. \n",
    "training.groupby('sentiment').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative     552\n",
       "neutral     1109\n",
       "positive    1422\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of instances that have each sentiment label in the development set, because if it does not have similar\n",
    "# layout as the training set the results might be misleading. The layout seems to be proportional.\n",
    "develop.groupby('sentiment').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some regural expression for pattern recognition so as to exclude or replace some words in the text.\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\", \"ain't\":\"is not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that gets the text of the tweet, cleans (urls, @names) or transforms (negations) it according to the patterns,\n",
    "# tokenizes it, makes all letters lowercase, removes punctuation and stop words,lemmatizes the words and returns a\n",
    "# cleaned string.\n",
    "\n",
    "def text_cleaning(text):\n",
    "\n",
    "    pulled_data = BeautifulSoup(text, 'lxml')\n",
    "    pulled_text = pulled_data.get_text()\n",
    "    stripped = re.sub(combined_pat, '', pulled_text)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], stripped)\n",
    "    tokens = nltk.word_tokenize(neg_handled)\n",
    "    lower_case = [word.lower() for word in tokens]\n",
    "    nonPunct = re.compile('.*[A-Za-z].*')\n",
    "    raw_words = [tok for tok in lower_case if nonPunct.match(tok)]\n",
    "    filtered_result = list(filter(lambda l: l not in new_stop_words, raw_words))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    final_string = (\" \".join(lemmas)).strip()\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the cleaning of the datasets and create an extra column to store the cleaned tweet.\n",
    "pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n",
    "training['cleaned_tweet'] = training.text.apply(text_cleaning)\n",
    "develop['cleaned_tweet'] = develop.text.apply(text_cleaning)\n",
    "test2016['cleaned_tweet'] = test2016.text.apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   sentiment  \\\n",
      "0  positive   \n",
      "3  negative   \n",
      "6  positive   \n",
      "7  negative   \n",
      "9  negative   \n",
      "\n",
      "                                                                                                                                               text  \\\n",
      "0  Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)                                                                                  \n",
      "3  Iranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)                      \n",
      "6  with J Davlar 11th. Main rivals are team Poland. Hopefully we an make it a successful end to a tough week of training tomorrow.                    \n",
      "7  Talking about ACT's &amp;&amp; SAT's, deciding where I want to go to college, applying to colleges and everything about college stresses me out.   \n",
      "9  They may have a SuperBowl in Dallas, but Dallas ain't winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll              \n",
      "\n",
      "                                                                                     cleaned_tweet  \n",
      "0  gas house hit 'm going chapel hill sat                                                           \n",
      "3  iranian general say israel 's iron dome not deal missile keep talking like may end finding       \n",
      "6  j davlar 11th main rival team poland hopefully make successful end tough week training tomorrow  \n",
      "7  talking act 's sat 's deciding want go college applying college everything college stress        \n",
      "9  may superbowl dallas dallas not winning superbowl not quarterback owner                           \n",
      "\n",
      "Development set:   sentiment  \\\n",
      "0  neutral    \n",
      "1  positive   \n",
      "2  neutral    \n",
      "4  positive   \n",
      "5  positive   \n",
      "\n",
      "                                                                                                                                           text  \\\n",
      "0  05 Beat it - Michael Jackson - Thriller (25th Anniversary Edition) [HD] http://t.co/A4K2B86PBv                                                 \n",
      "1  Jay Z joins Instagram with nostalgic tribute to Michael Jackson: Jay Z apparently joined Instagram on Saturday and.. http://t.co/Qj9I4eCvXy    \n",
      "2  Michael Jackson: Bad 25th Anniversary Edition (Picture Vinyl): This unique picture disc vinyl includes the original 1 http://t.co/fKXhToAAuW   \n",
      "4  18th anniv of Princess Diana's death. I still want to believe she is living on a private island away from the public. With Michael Jackson.    \n",
      "5  @oridaganjazz The 1st time I heard Michael Jackson sing was in Honolulu, Hawaii @ a restaurant on radio. It was A.B.C. I was 13. I loved it!   \n",
      "\n",
      "                                                                                             cleaned_tweet  \n",
      "0  beat michael jackson thriller 25th anniversary edition hd                                                \n",
      "1  jay z join instagram nostalgic tribute michael jackson jay z apparently joined instagram saturday and..  \n",
      "2  michael jackson bad 25th anniversary edition picture vinyl unique picture disc vinyl includes original   \n",
      "4  18th anniv princess diana 's death still want believe living private island away public michael jackson  \n",
      "5  1st time heard michael jackson sing honolulu hawaii restaurant radio a.b.c loved                          \n",
      "\n",
      "Test set:   sentiment  \\\n",
      "0  neutral    \n",
      "1  neutral    \n",
      "3  neutral    \n",
      "4  positive   \n",
      "5  positive   \n",
      "\n",
      "                                                                                                                                           text  \\\n",
      "0  Picturehouse's, Pink Floyd's, 'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...     \n",
      "1  Order Go Set a Watchman in store or through our website before Tuesday and get it half price! #GSAW @GSAWatchmanBook https://t.co/KET6EGD1an   \n",
      "3  If you could ask an onstage interview question at Miss USA tomorrow, what would it be?                                                         \n",
      "4  A portion of book sales from our Harper Lee/Go Set a Watchman release party on Mon. 7/13 will support @CAP_Tulsa and the great work they do.   \n",
      "5  Excited to read \"Go Set a Watchman\" on Tuesday.  But can it possibly live up to \"To Kill a Mockingbird?\"  Any opinions?                        \n",
      "\n",
      "                                                                                                cleaned_tweet  \n",
      "0  picturehouse 's pink floyd 's 'roger water walll opening sept making wave watch trailer rolling stone look  \n",
      "1  order go set watchman store website tuesday get half price gsaw                                             \n",
      "3  could ask onstage interview question miss usa tomorrow would                                                \n",
      "4  portion book sale harper lee/go set watchman release party mon support great work                           \n",
      "5  excited read go set watchman tuesday possibly live kill mockingbird opinion                                 \n"
     ]
    }
   ],
   "source": [
    "# Check if everything went well with the data cleaning\n",
    "print \"Training set:\", training[['sentiment','text','cleaned_tweet']].head(), \"\\n\"\n",
    "print \"Development set:\", develop[['sentiment','text','cleaned_tweet']].head(), \"\\n\"\n",
    "print \"Test set:\", test2016[['sentiment','text','cleaned_tweet']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Training:\n",
      "Empty DataFrame\n",
      "Columns: [sentiment, text, cleaned_tweet]\n",
      "Index: [] \n",
      "\n",
      "Null Develop:\n",
      "Empty DataFrame\n",
      "Columns: [sentiment, text, cleaned_tweet]\n",
      "Index: [] \n",
      "\n",
      "Null Test:\n",
      "Empty DataFrame\n",
      "Columns: [sentiment, text, cleaned_tweet]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Make sure nothing is left null after the cleaning of the text.\n",
    "print \"Null Training:\\n\", training[training.isnull().any(axis=1)].head(), \"\\n\"\n",
    "print \"Null Develop:\\n\", develop[develop.isnull().any(axis=1)].head(), \"\\n\"\n",
    "print \"Null Test:\\n\", test2016[test2016.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Feature Engineering\n",
    "\n",
    "##### Feature selection/feture engineering (as in new features)/data transformations. \n",
    "\n",
    "A form of feature selection is already been done by lemmatizing the words and removing stop words and other things that does not seem necessary in the cleaning stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the vectors out of the words using count vecrtorizer. We transform the data so that it can be the input for the \n",
    "# algorithms.\n",
    "vectorized_data = count_vectorizer.fit_transform(training.cleaned_tweet)\n",
    "vectorized_develop = count_vectorizer.transform(develop.cleaned_tweet)\n",
    "vectorized_test = count_vectorizer.transform(test2016.cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the vectors out of the words using tfidf vecrtorizer.\n",
    "tfidf_vectorized_data = tfidf_vectorizer.fit_transform(training.cleaned_tweet)\n",
    "tfidf_vectorized_develop = tfidf_vectorizer.transform(develop.cleaned_tweet)\n",
    "tfidf_vectorized_test = tfidf_vectorizer.transform(test2016.cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the sentiment (positive, negative, neutral) in numerical form to be able to compare the results easily.\n",
    "def sentiment2target(sentiment):\n",
    "    return {\n",
    "        'negative': 0,\n",
    "        'neutral': 1,\n",
    "        'positive' : 2\n",
    "    }[sentiment]\n",
    "\n",
    "training_targets = training.sentiment.apply(sentiment2target)\n",
    "develop_targets = develop.sentiment.apply(sentiment2target)\n",
    "test_targets = test2016.sentiment.apply(sentiment2target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Algorithm Selection\n",
    "\n",
    "##### Select a set of algorithms to apply, select evaluation metrics, and evaluate/compare algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compare a set of algorithms, regarding their accuracy:\n",
    "1. Random Forests\n",
    "2. K-Nearest Neighbors\n",
    "3. LinearSVC\n",
    "4. Multinomial Naive Bayes\n",
    "5. Logistic Regression\n",
    "6. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation metric:accuracy\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RF: 0.603856 (+/- 0.081373)\n",
      "KNN: 0.447209 (+/- 0.075410)\n",
      "SVM: 0.617535 (+/- 0.070093)\n",
      "MNB: 0.594719 (+/- 0.040343)\n",
      " LR: 0.624750 (+/- 0.072598)\n",
      " DT: 0.595214 (+/- 0.077542)\n"
     ]
    }
   ],
   "source": [
    "# Perform 10-fold cross-validation on training set to find the algorithm with the best performance. Even though we have\n",
    "# development set, we choose to perform this cross-validation and use the development set to tune the hyperparameters \n",
    "# of the chosen model. We try firstly with the count vectorized set and then with the tfidf vectorized set.  \n",
    "\n",
    "# count vectorizer\n",
    "models = []\n",
    "models.append(('RF',  RandomForestClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVM', LinearSVC()))\n",
    "models.append(('MNB', MultinomialNB()))\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('DT',  DecisionTreeClassifier()))\n",
    "\n",
    "results = []\n",
    "names   = []\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, vectorized_data, training_targets, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"%03s: %f (+/- %f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RF: 0.599578 (+/- 0.076483)\n",
      "KNN: 0.510566 (+/- 0.025548)\n",
      "SVM: 0.622485 (+/- 0.070648)\n",
      "MNB: 0.582808 (+/- 0.033425)\n",
      " LR: 0.603607 (+/- 0.070324)\n",
      " DT: 0.587244 (+/- 0.071337)\n"
     ]
    }
   ],
   "source": [
    "#tfidf vectorizer\n",
    "models = []\n",
    "models.append(('RF',  RandomForestClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVM', LinearSVC()))\n",
    "models.append(('MNB', MultinomialNB()))\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('DT',  DecisionTreeClassifier()))\n",
    "\n",
    "results = []\n",
    "names   = []\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, tfidf_vectorized_data, training_targets, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"%03s: %f (+/- %f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Training\n",
    "\n",
    "##### Apply ensembles and improve performance by hyperparameter optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'squared_hinge', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Try to find the best hyperparameters for LinearSVC which we chose.\n",
    "params = {'C': [0.01, 0.1, 1, 10, 100], 'loss': ['hinge', 'squared_hinge']}\n",
    "linearSVC = GridSearchCV(LinearSVC(), params, scoring='accuracy')\n",
    "linearSVC.fit(tfidf_vectorized_develop, (np.array(develop_targets)).ravel())\n",
    "print linearSVC.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 8: Finalise Model\n",
    "\n",
    "##### Predictions on validation set, create model from the entire (training) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data is 0.593444322083\n"
     ]
    }
   ],
   "source": [
    "linearSVC = LinearSVC(loss='squared_hinge', C=0.1)\n",
    "model = linearSVC.fit(tfidf_vectorized_data, training_targets)\n",
    "pred = model.predict(tfidf_vectorized_test)\n",
    "print \"Accuracy for test data is\", accuracy_score(test_targets, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Procedure and choices made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading and exploring the datasets, we perform cleaning of the tweets' text using some regular expressions and other useful variables. We choose to clean the text from urls, @names and transform negations according to the patterns, because these are not useful for sentiment analysis. For the same reason and to reduce the number of features we also remove stop words and perform lemmatization.\n",
    "\n",
    "Then, we choose to vectorize the features left, on the one hand with the count vectorizer and on the other hand with the tfidf vectorizer, to check which one makes each algorithm performs better. We also transform the sentiment of the tweets by enumerating them in range [0-2] to help the algorithms with the calculations.\n",
    "\n",
    "Then, we perform algorithm selection by using 10-fold cross-validation on the training set to find the algorithm with the best performance. Even though we have development set, we choose to use it only to tune the hyperparameters of the chosen model. The algorithms that we choose to try are: RandomForestClassifier, KNeighborsClassifier, LinearSVC, MultinomialNB, LogisticRegression and DecisionTreeClassifier.\n",
    "\n",
    "As we can observe from the results of the cross-validation for the two vectorized training data, LogisticRegression produces the best mean accuracy for count vectorized data and LinearSVC produces the best mean accuracy for tfidf vectorized data. Because of the fact that between the two of them, the LinearSVC produces an outcome with less standard deviation, we choose this algorithm for the next step which is the tuning of the hyperparameters.\n",
    "\n",
    "We use grid search to find the optimal hyperparameters. We choose to tune the constant C which is the penalty parameter of the error term and the loss function. The best hyperparameters are C = 0.1 and loss = square hinge. \n",
    "\n",
    "Finally, we train the algorithm with these specific hyperparameters and test the model with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
